{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvNeuralNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL6bWEQ933Pl"
      },
      "source": [
        "#import required packages\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "#important hyperparameters\n",
        "BATCH_S = 30\n",
        "EPOCHS = 50\n",
        "STEPS_PER_EPOCH = 20\n",
        "VALIDATION_STEPS = 10\n",
        "\n",
        "#construct the dataset and the labels array \n",
        "imagePaths = list(paths.list_images(\"../banana_images\"))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# loop over the image paths\n",
        "for imagePath in imagePaths:\n",
        "\tfilename_w_ext = os.path.basename(imagePath)\n",
        "  filename, file_extension = os.path.splitext(filename_w_ext)\n",
        "  if \"0\" in filename:\n",
        "    label = 0\n",
        "  else:\n",
        "    label = 1\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = cv2.resize(image, (200, 200), interpolation = cv2.INTER_AREA)\n",
        "\t\n",
        "  #add these findings to the respective arrays \n",
        "\tdata.append(image)\n",
        "\tlabels.append(label)\n",
        "\n",
        "#normalising pixel values\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "\n",
        "#one-hot encoding the labels\n",
        "labels = np_utils.to_categorical(labels, 2)\n",
        "\n",
        "# use an argument passer for data pre-processing\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "\thelp=\"input path\")\n",
        "ap.add_argument(\"-o\", \"--output\", required=True,\n",
        "\thelp=\"directory to store the outputted augmented images\")\n",
        "ap.add_argument(\"-t\", \"--total\", type=int, default=600,\n",
        "\thelp=\"how many samples?\")\n",
        "args = vars(ap.parse_args())\n",
        "\n",
        "#image pre-processing\n",
        "def processing(data):\n",
        "  for image in data: \n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    aug = ImageDataGenerator(\n",
        "      rotation_range=30,\n",
        "      zoom_range=0.15,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.15,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode=\"nearest\")\n",
        "    total = 0\n",
        "\n",
        "    imageGen = aug.flow(image, batch_size=BATCH_S, save_to_dir=args[\"output\"],\n",
        "      save_prefix=\"image\", save_format=\"jpg\")\n",
        "    # loop over examples from our image data augmentation generator\n",
        "    for image in imageGen:\n",
        "      # increment our counter\n",
        "      total += 1\n",
        "      # if we have reached the specified number of examples, break\n",
        "      # from the loop\n",
        "      if total == args[\"total\"]:\n",
        "        break\n",
        "#partition of data into training and testing (used for validation)\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(processedData, labels,\n",
        "\ttest_size=0.33, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sEdzM8v4ER0"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, kernel_size=3, activation= 'relu', input_shape=(200,200,3)))\n",
        "model.add(Conv2D(64, kernel_size=3, activation= 'relu', input_shape=(100,100,3)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation= 'relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation= 'softmax'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaaurcprDIZY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH3znahFDIQq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6plIlldX639z"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsdpwDIM66hK"
      },
      "source": [
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHS, steps_per_epoch = STEPS_PER_EPOCH, validation_steps = VALIDATION_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-hZM4SyPhI5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHS)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rKE65HQ67Ek"
      },
      "source": [
        "#transfer learning for improved accuracy\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "#freezing all the pre-trained weights, removing the top layer to replace my model layers\n",
        "restnet = ResNet50(include_top=False, weights='imagenet', input_shape=(200,200,3))\n",
        "output = restnet.layers[-1].output\n",
        "output = keras.layers.Flatten()(output)\n",
        "restnet = Model(restnet.input, output=output)\n",
        "for layer in restnet.layers:\n",
        "    layer.trainable = False\n",
        "#understand the structure\n",
        "resnet.summary()\n",
        "#building a new model\n",
        "model = Sequential()\n",
        "model.add(restnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4If9f6rt7pW6"
      },
      "source": [
        "#gaussian blur \n",
        "reduce_noise = []\n",
        "for i in range(len(dataset)):\n",
        "    blur = cv2.GaussianBlur(transImgs[i], (10, 10), 0)\n",
        "    reduce_noise.append(blur)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG29Al9o-DP5"
      },
      "source": [
        "#image segmentation (k-means clustering)\n",
        "from PIL import Image\n",
        "from sklearn.cluster import KMeans\n",
        "Segpics = []\n",
        "for i in range(0,len(reduce_noise)):\n",
        "  pics_n = np.zeros( (200,200,3), dtype=np.uint8)\n",
        "  for j in range(0,len(pics_n)):\n",
        "    for p in range(0,len(pics_n)):\n",
        "      pics_n[j][p] = (reduce_noise[j][p]*255.0)\n",
        "  recon_pic = Image.fromarray(pics_n,'RGB')\n",
        "  recon_pic_n = pic.reshape(recon_pic.shape[0]*recon_pic.shape[1], recon_pic.shape[2])\n",
        "  kmeans = KMeans(n_clusters=3, random_state=0).fit(pic_n)\n",
        "  pic2show = kmeans.cluster_centers_[kmeans.labels_]\n",
        "  cluster_pic = pic2show.reshape(recon_pic.shape[0], recon_pic.shape[1], recon_pic.shape[2])\n",
        "  Segpics.append(cluster_pic)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6KzHhYhF393"
      },
      "source": [
        "#sample image segmentation\n",
        "import os, sys\n",
        "from os import listdir\n",
        "\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# need to enter password to access your google drive\n",
        "\n",
        "from google.colab import files\n",
        "main_dir = \"/content/gdrive/My Drive/banana_imgs/\"\n",
        "files = listdir(main_dir)\n",
        "# you can change file extension below to read other image types\n",
        "images_list = [i for i in files if i.endswith('.jpg')] ## output file names only\n",
        "from sklearn.cluster import KMeans\n",
        "for idx,image in enumerate(images_list):\n",
        "  pic = plt.imread(main_dir + image)/255\n",
        "  plt.imshow(pic)\n",
        "  pic_n = pic.reshape(pic.shape[0]*pic.shape[1], pic.shape[2])\n",
        "  kmeans = KMeans(n_clusters=5, random_state=0).fit(pic_n)\n",
        "  pic2show = kmeans.cluster_centers_[kmeans.labels_]\n",
        "  cluster_pic = pic2show.reshape(pic.shape[0], pic.shape[1], pic.shape[2])\n",
        "  plt.imshow(cluster_pic)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}